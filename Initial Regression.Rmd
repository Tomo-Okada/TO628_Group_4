---
title: "Career Consultation and Prediciting Data Science Compensation"
author: "Ariella Rose, Benjamin Lewis, Rahul Jain, Yoshitomo Okada"
date: "4/11/2021"
output: 
  html_document:
    highlight: pygments
    theme: sandstone

---


# {.tabset}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE, echo=FALSE}
# load libraries
library(gmodels)
library(e1071)
library(caret)
library(class)
library(C50)
library(neuralnet)
library(irr)
```

## Introduction


### Welcome to Twerk From Home

Twerk From Home is a Career Consulting Company that helps clients take the next steps towards a successful career. We do the career planning, while you *Twerk From Home!*

A huge factor in deciding one's next career move is compensation. In this report, we target clients who want to transition into, or continue, a career in Data Science. We highlight the factors that enable data professionals to make above average salaries, greater than **$ 64,750.60**, using the data set, “How much does data scientists earn in 2017-2020,” obtained from Stack overflow. We begin by running a logistic regression model to ascertain which variables are significant in increasing someone's likelihood of making an above average salary, and use this information to inform our clients on what decisions they should make to ensure they reach their salary goal. Afterwards, we employ several other predictive models to improve predictive power and accuracy.

Finally, we end with some discussion about our analysis and give recommendations to our clients based on our output on how to secure a high paying data science position. 


## Load and Clean Data
```{r}

#Read Data
dssalaries <- read.csv("processed_data_toDummies.csv")
nrow(dssalaries)
str(dssalaries)
summary(dssalaries)

#Convert Year to Factor
dssalaries$Year <- as.factor(dssalaries$Year)

#Convert Hobbyist to Factor
dssalaries$Hobbyist <- as.factor(dssalaries$Hobbyist)

#Combine all Hobbyist Yes answers to a single factor, leaving two factors for Hobbyist: Yes and No
levels(dssalaries$Hobbyist) <- c("No","Yes","Yes","Yes","Yes")

#Convert country to factor
dssalaries$Country <- as.factor(dssalaries$Country)

#Convert EdLevel to factor
dssalaries$EdLevel <- as.factor(dssalaries$EdLevel)

#Remove all rows missing EdLevel
dssalaries <- droplevels(dssalaries[!dssalaries$EdLevel=='',])

#Convert Employment to factor
dssalaries$Employment <- as.factor(dssalaries$Employment)

#Remove all rows missing Employment
dssalaries <- droplevels(dssalaries[!dssalaries$Employment=='',])

#Replace JobSat NA's with the mean
dssalaries$JobSat <- ifelse(is.na(dssalaries$JobSat),mean(dssalaries$JobSat,na.rm=TRUE),dssalaries$JobSat)

#Convert OrgSize to factor
dssalaries$OrgSize <- as.factor(dssalaries$OrgSize)

#Remove all rows missing OrgSize
dssalaries <- droplevels(dssalaries[!dssalaries$OrgSize=='',])
dssalaries <- droplevels(dssalaries[!dssalaries$OrgSize=='I prefer not to answer',])
dssalaries <- droplevels(dssalaries[!dssalaries$OrgSize=="I don't know",])

#Convert UndergradMajor to factor
dssalaries$UndergradMajor <- as.factor(dssalaries$UndergradMajor)

#Remove all rows missing UndergradMajor
dssalaries <- droplevels(dssalaries[!dssalaries$UndergradMajor=='',])

#Replace YearsCodePro NA's with the mean
dssalaries$YearsCodePro <- ifelse(is.na(dssalaries$YearsCodePro),mean(dssalaries$YearsCodePro,na.rm=TRUE),dssalaries$YearsCodePro)

#Remove all rows with any remaining NA values
dssalaries <- dssalaries[complete.cases(dssalaries),]

#Remove Years that no longer have any data points
dssalaries <- droplevels(dssalaries[!dssalaries$Year=='2017',])
dssalaries <- droplevels(dssalaries[!dssalaries$Year=='2018',])

#Create new column for salary above mean
dssalaries$salaryabove <- ifelse(dssalaries$ConvertedComp > mean(dssalaries$ConvertedComp),1,0)

#Combine some of the EdLevel factors
levels(dssalaries$EdLevel) <- c("Basic","Basic","Doctorate","Basic","Professionl","Basic")

#Combine some of the Employment factors
levels(dssalaries$Employment) <- c("Full","Part","Full")

#Combine some of the OrgSize factors
levels(dssalaries$OrgSize) <- c("500-10K","0-500","10K+","0-500","0-500","0-500","500-10K","500-10K","0-500")

# nULL uneeded variables
dssalaries$ConvertedComp <- NULL
dssalaries$Country <- NULL

#Combine some of the UndergradMajor factors
#levels(dssalaries$UndergradMajor) <- c("Eng+CS+IT","Bus","Eng+CS+IT","Art+Hum","Science","Art+Hum","None","Eng+CS+IT","Math","Science","Science","Eng+CS+IT")

```
<!-- ## Creating Test and Train Data Sets -->
<!-- # ```{r} -->
<!-- # set.seed(12345) -->
<!-- # test_set <- sample(1:nrow(dssalaries),4000) -->
<!-- #  -->
<!-- # # #Creating test and train data sets -->
<!-- # # dssalariestest <- dssalaries[test_set,] -->
<!-- # # dssalariestrain <- dssalaries[-test_set,] -->
<!-- #  -->
<!-- # ``` -->



<!-- ## Logistic Regression Model -->
<!-- ```{r} -->
<!-- model <- glm(salaryabove ~Hobbyist+EdLevel+Employment+JobSat+OrgSize+UndergradMajor+YearsCodePro+Data.scientist.or.machine.learning.specialist+Database.administrator+Data.or.business.analyst+Engineer..data, data=dssalariestrain, family= "binomial") -->
<!-- summary(model) -->

<!-- model1 <- glm(salaryabove ~Hobbyist+EdLevel+Employment+JobSat+OrgSize+UndergradMajor+YearsCodePro+Database.administrator+Data.or.business.analyst+Engineer..data, data=dssalariestrain, family= "binomial") -->
<!-- summary(model1) -->

<!-- ``` -->
<!-- ## Prediction and Results -->
<!-- ```{r} -->
<!-- #Logistic Regression Prediction -->
<!-- results <- predict(model1, dssalariestest, type = "response") -->
<!-- summary(results) -->

<!-- #Converting regression predictions to binary -->
<!-- binresults <- ifelse(results< 0.5, 0,1) -->

<!-- #Logistic Regression Results -->
<!-- CrossTable(x = binresults, y = dssalariestest$salaryabove, prop.chisq=FALSE) -->
<!-- logresults<-confusionMatrix(as.factor(binresults),as.factor(dssalariestest$salaryabove),positive="1") -->
<!-- logresults -->

<!-- ``` -->

## Getting Data Ready for Analysis

```{r, cache=T}
#Convert all variables to dummies
dssalaries_m <- as.data.frame(model.matrix(~.-1,dssalaries))
str(dssalaries_m)
summary(dssalaries_m)

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
dssalaries_norm <- as.data.frame(lapply(dssalaries_m, normalize))
summary(dssalaries_norm)

# create training and testing dataset with normalized data
set.seed(12345)
test_set <- sample(1:nrow(dssalaries_norm),4000)

#Creating test and train data sets
dssalariestest_norm <- dssalaries_norm[test_set,]
dssalariestrain_norm <- dssalaries_norm[-test_set,]


```

## Logistic Regression Model
```{r}
#Logistic Regression Model using all variables
model <- glm(salaryabove ~., data=dssalariestrain_norm, family= "binomial")
summary(model)

#Logistic Regression Model removing insignificant variables
model1 <- glm(salaryabove ~ Year2019+HobbyistYes+EdLevelDoctorate+EdLevelProfessionl+EmploymentPart+JobSat+OrgSize0.500+OrgSize10K.+UndergradMajorBusiness+UndergradMajorFine.arts.or.performing.arts+UndergradMajorHumanities+UndergradMajorI.never.declared.a.major+UndergradMajorMathematics.or.statistics+UndergradMajorNatural.science+UndergradMajorSocial.science+UndergradMajorWeb.development.or.web.design+YearsCodePro+Database.administrator+Data.or.business.analyst+Engineer..data, data=dssalariestrain_norm, family= "binomial")
summary(model1)

#Logistic Regression Prediction
results <- predict(model1, dssalariestest_norm, type = "response")
summary(results)

#Converting regression predictions to binary
binresults <- ifelse(results< 0.5, 0,1)

#Logistic Regression Results Confusion Matrix
#CrossTable(x = binresults, y = dssalariestest_norm$salaryabove, prop.chisq=FALSE)
logresults<-confusionMatrix(as.factor(binresults),as.factor(dssalariestest_norm$salaryabove),positive="1")
logresults

```
<!-- ## Prediction and Results -->
<!-- ```{r} -->
<!-- #Logistic Regression Prediction -->
<!-- results <- predict(model1, dssalariestest_norm, type = "response") -->
<!-- summary(results) -->

<!-- #Converting regression predictions to binary -->
<!-- binresults <- ifelse(results< 0.5, 0,1) -->

<!-- #Logistic Regression Results -->
<!-- CrossTable(x = binresults, y = dssalariestest$salaryabove, prop.chisq=FALSE) -->
<!-- logresults<-confusionMatrix(as.factor(binresults),as.factor(dssalariestest$salaryabove),positive="1") -->
<!-- logresults -->

<!-- ``` -->



## KNN Model
```{r, cache=T}
dssalaries_norm_n<-dssalaries_norm[-which(colnames(dssalaries_norm)=="salaryabove" )]

#Create labels for KNN
dssal_knn_train<-dssalariestrain_norm[-which(colnames(dssalaries_norm)=="salaryabove" )]
dssal_knn_test<-dssalariestest_norm[-which(colnames(dssalaries_norm)=="salaryabove" )]
dssal_knn_train_label<-dssalariestrain_norm[,which(colnames(dssalaries_norm)=="salaryabove" )]
dssal_knn_test_label<-dssalariestest_norm[,which(colnames(dssalaries_norm)=="salaryabove" )]

#KNN Model with k=119
dssal_knn_test_pred <- knn(train = dssal_knn_train, test = dssal_knn_test,
                      cl = dssal_knn_train_label, k=round(sqrt(nrow(dssalariestrain_norm))))

#KNN Model Results Confusion Matrix
confusionMatrix(as.factor(dssal_knn_test_pred),as.factor(dssalariestest_norm$salaryabove),positive = "1")

#KNN Model with k=13
dssal_knn_test_pred1 <- knn(train = dssal_knn_train, test = dssal_knn_test,
                      cl = dssal_knn_train_label, k=13)
#KNN Model Results Confusion Matrix
knnresults <- confusionMatrix(as.factor(dssal_knn_test_pred1),as.factor(dssalariestest_norm$salaryabove),positive = "1")

```

## ANN Model

```{r, cache=T}

# simple ANN with only a single hidden neuron
dssal_ann_model <- neuralnet(formula = salaryabove ~ .,
                              data = dssalariestrain_norm)


# visualize the network topology
plot(dssal_ann_model)

## Step 4: Evaluating model performance ----
# obtain model results
model_results <- compute(dssal_ann_model, dssalariestest_norm[-which(colnames(dssalaries_norm)=="salaryabove" )])
# obtain predicted strength values
predicted_sal <- model_results$net.result

#Convert results to binary
predicted_sal2<-ifelse(predicted_sal > 0.5,1,0)

#ANN model results Confusion Matrix
annresults <- confusionMatrix(as.factor(predicted_sal2),as.factor(dssalariestest_norm$salaryabove),positive = "1")


```

## Decision Tree
```{r}
#Convert dependent variable to a factor
dssalariestrain_norm$salaryabove <- as.factor(dssalariestrain_norm$salaryabove)
dssalariestest_norm$salaryabove <- as.factor(dssalariestest_norm$salaryabove)

#Decision tree model
dssal_DT_model <- C5.0(salaryabove ~ ., data = dssalariestrain_norm)
plot(dssal_DT_model)
summary(dssal_DT_model)

#Decision Tree Model prediction
dssal_DT_predict <- predict(dssal_DT_model, dssalariestest_norm)

#Decision Tree Model results Confusion Matrix
orig<-confusionMatrix(as.factor(dssal_DT_predict),as.factor(dssalariestest_norm$salaryabove),positive = "1")
orig

# ## cost model
# error_cost <- matrix(c(0,2,1,0), nrow=2)
# error_cost
# 
# dssal_DT_cost_model <- C5.0(salaryabove ~ ., data = dssalariestrain_norm, costs = error_cost)
# dssal_DT_cost_predict <- predict(dssal_DT_cost_model, dssalariestest_norm)
# 
# cost<-confusionMatrix(as.factor(dssal_DT_cost_predict),as.factor(dssalariestest_norm$salaryabove),positive = "1")
# cost
```

## Stacked Model
```{r}
# Create Predictions Dataframe
combined<-data.frame(ann=as.numeric(as.character(predicted_sal2)),knn=as.numeric(as.character(dssal_knn_test_pred1)),log=as.numeric(as.character(binresults)),dt=as.numeric(as.character(dssal_DT_predict)), actual=dssalariestest_norm$salaryabove)
#combined

# Separate data to train and test
combined_test <- combined[1:1000,]
combined_train <- combined[1001:4000,]

# Create stacked model
stacked<-C5.0(actual~. ,data=combined_train)
plot(stacked)
summary(stacked)

# Stacked model prediction
final_predict <- predict(stacked, combined_test)

#Stacked model results Confusion Matrix
stackedresults <- confusionMatrix(as.factor(final_predict),as.factor(combined_test$actual),positive = "1")

```

<!-- ## Automated 10 Fold CV -->

<!-- ```{r} -->
<!-- set.seed(1234) -->
<!-- folds <- createFolds(combined$actual, k = 10) -->

<!-- cv_results <- lapply(folds, function(x) { -->
<!--   combined_train <- combined[-x, ] -->
<!--   combined_test <- combined[x, ] -->
<!--   combined_model <- C5.0(actual ~ ., data = combined_train) -->
<!--   combined_pred <- predict(combined_model, combined_test) -->
<!--   combined_actual <- combined_test$actual -->
<!--   kappa <- kappa2(data.frame(combined_actual, combined_pred))$value -->
<!--   return(kappa) -->
<!-- }) -->

<!-- str(cv_results) -->
<!-- mean(unlist(cv_results)) -->
<!-- ``` -->

## Tuned Model
```{r}
## Customizing the tuning process ----
# use trainControl() to alter resampling strategy
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

# use expand.grid() to create grid of tuning parameters
grid <- expand.grid(.model = "tree",
                    .trials = c(1, 2, 3, 4, 5),
                    .winnow = "FALSE")

# look at the result of expand.grid()
grid

# customize train() with the control list and grid of parameters 
set.seed(300)
m <- train(actual ~ ., data = combined, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
m
```


## Discussion

### Analysis scope
Using the data set, “How much does data scientists earn in 2017-2020,” which is the dataset processed from the Stack overflow Annual Developers Survey, we analyzed and predicted how data scientists expect to earn above mean salary among this data set which is $64.75k (Salary converted to annual USD salaries using the exchange rate on each year, assuming 12 working months and 50 working weeks.). 

### Logistic Regression Model results
According to the Logistic Regression Model results, we could find out some crucial factors for a higher salary among data-related jobs.
For instance, an education level that is the highest level of formal education that candidates have completed shows interesting insights. The Professional degree (JD, MD, etc., not included MBA) shows a statistically lower probability of gaining above mean salary than the Basic (Master, Bachelor, or lower). In contrast, Doctorate education has a positive impact on their salary. We interpreted this result that a professional degree is not common in the data-related business field yet. High education in different areas is not necessary to lead to higher compensation in data-related jobs.
Also, we confirmed that a full-time job offers a significantly higher salary than a part-timer, independent contractor, freelancer, or self-employed data scientist.
We can also say that organizations which employ more than 10k show higher salary. Which means that even though job offer from venture company seems attractive about challenging opportunity, future skill growth, and right of autonomy, the overall well-established company would offer stable compensation for data-related jobs.
One exciting finding is that undergrad majors in Computer science, Information systems, or Web development/web design are not statistically positive factors for a higher salary of data-related jobs. This seems counter to our intuition, but we concluded that data-related skills are required in various business fields. In other words, the person who can mash up a specialty in each field and data-related skills are scarce in job markets, and they gain higher salary than the specialized person only in typical and traditional computer science fields.
The dataset has the following four specific career types among data-related jobs. 

* Data Scientist or Machine Learning Specialist
* Database Administrator
* Data Analyst
* Business Analyst and Data Engineer

The regression results indicate that Business Analyst and Data Engineer has the highest probability to gain above mean salary. In comparison, Database Administrator and Data Analyst is lower probability than Data Scientist or Machine Learning Specialist. This means that we can expect data scientists to earn above mean salaries than traditional data-related jobs. However, we need to recognize that Business Analyst who utilizes data skills in consulting or Data Engineer who creates new products in the upstream process would earn more than Data Scientist or Machine Learning Specialist.
The regression results also show that the number of years of coding experience has the largest impact on salary. More years of coding experience increases the likelihood of earning above the mean salary by 3 times the other variables considered. Clearly, experience is valued as a data scientist and experience leads to greater compensation. The best way to earn more as a data scientist may be to continue to do it for many years.

Overall, the logistic regression model using variables with only 0.05 or less significance results in an accuracy of **`r round(logresults$overall['Accuracy'],4)*100`**% and a kappa of **`r round(logresults$overall['Kappa'],3)`**.

### KNN Model results
Various K values were tested to determine the best performing KNN model, starting with the square root of the number of rows (119). The best performing model based on kappa was found at k=13. The model results in an accuracy of **`r round(knnresults$overall['Accuracy'],4)*100`**% and a kappa of **`r round(knnresults$overall['Kappa'],3)`**.

### ANN Model results
A simple ANN model with one hidden node was created and results in an accuracy of **`r round(annresults$overall['Accuracy'],4)*100`**% and a kappa of **`r round(annresults$overall['Kappa'],3)`**. Various other numbers of hidden nodes were tested and did not result in significantly better results for the added complexity.

### Decision Tree Model results
A decision tree model showed similar variables of importance as the logistic regression model. The variable used in 100% of decisions is the years of professional coding experience showing its significance to determining salary. The following most used variables are part time versus full time employment and the organization size as discussed previously. Job satisfaction also plays an important role being used in 85.79% of decisions. Further investigation could be performed to understand the relationship between job satisfaction and salary, which could be correlated or causal. Overall, the decision tree model results in an accuracy of **`r round(orig$overall['Accuracy'],4)*100`**% and a kappa of **`r round(orig$overall['Kappa'],3)`**.

### Stacked Model results
We constructed a second level stacked model. The ANN is the best performing model with an Accuracy of **`r round(annresults$overall['Accuracy'],4)*100`**% and the highest Kappa of **`r round(annresults$overall['Kappa'],3)`** among individual models. The second level combined model uses only the ANN model due to its high performing nature. The second level combined model could not improve Accuracy and Kappa which is **`r round(stackedresults$overall['Accuracy'],4)*100`**% and a kappa of **`r round(stackedresults$overall['Kappa'],3)`**.

### Model Tuning
Finally, we tuned the second level model using 10 cross validation and various trials for decision tree models. Using 10 fold cross validation reduces over-fitting and increases the reliability of the result. The results of all combinations are similar to results of the original ANN algorithm and better than the single stacked model created. This result shows the reliability of the model using various test and train sets despite the initial stacked model being slightly worse. 3 trials with no winnowing gives us the best performing model based on kappa values and oneSE. Higher kappa values are achieved with higher number of trials, but the value gained does not out-weigh the added complexity of the additional trials. 

### Recommendation
Based on these results, we concluded that we could utilize this model, especially the ANN model, to predict career success in terms of gaining salary above mean in the areas such as career development center in university or a job-changing advisory service. Though Data Scientist or Machine Learning Specialist career seems attractive among data-related jobs, to secure a higher salary, we would advise candidates to broaden their skill sets to business fields or aim for an engineering position in the upstream process.


Data source: https://www.kaggle.com/phuchuynguyen/datarelated-developers-survey-by-stack-overflow?select=processed_data_toDummies.csv
